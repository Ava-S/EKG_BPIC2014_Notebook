{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:50:48.959891Z",
     "start_time": "2025-05-31T15:50:48.845710Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from promg.modules.db_management import DBManagement\n",
    "from tabulate import tabulate\n",
    "import yaml\n",
    "\n",
    "from promg import Configuration, DatabaseConnection, Performance, SemanticHeader, DatasetDescriptions, OcedPg, Query\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.width', 2000)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Define the project that you want to do analysis on",
   "id": "53749a6e25cc2172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:50:50.382523Z",
     "start_time": "2025-05-31T15:50:50.378699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "case_study = 'bpic14'\n",
    "# case_study = 'bpic17'\n",
    "use_sample = False"
   ],
   "id": "fc9da47c43b8414d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:50:50.979600Z",
     "start_time": "2025-05-31T15:50:50.950008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# retrieve configuration for case_study\n",
    "conf_path = Path(case_study, 'config.yaml')\n",
    "config = yaml.safe_load(open(conf_path))\n",
    "\n",
    "print(f\"These are the credentials that I expect to be set for the database.\")\n",
    "print(f\"db_name: {config[\"db_name\"]}\")\n",
    "print(f\"uri: {config[\"uri\"]}\")\n",
    "print(f\"password: {config[\"password\"]}\")\n",
    "print(\"----------------------\")\n",
    "print(f\"If you have other credentials, please change them at: {conf_path}\")"
   ],
   "id": "aa3387e167d67135",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the credentials that I expect to be set for the database.\n",
      "db_name: neo4j\n",
      "uri: bolt://localhost:7687\n",
      "password: bpic2014\n",
      "----------------------\n",
      "If you have other credentials, please change them at: bpic14\\config.yaml\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare so we can use PromG to load the data and execute queries",
   "id": "e8120ab6c716ab32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:50:53.136404Z",
     "start_time": "2025-05-31T15:50:53.102297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = Configuration.init_conf_with_config_file(conf_path)\n",
    "db_connection = DatabaseConnection.set_up_connection(config=config)"
   ],
   "id": "6a43b12f667191c7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the data",
   "id": "fc848247a6c26bb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Start Analysis\n",
    "Make sure the data imported using import_data.ipynb. The imported data is in the schema as defined in the paper"
   ],
   "id": "8de33f54a3ecfdef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analysis Goals = Process Discovery\n",
    "Let's discover the process"
   ],
   "id": "a2e7cf94d6deb4ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:50:55.179961Z",
     "start_time": "2025-05-31T15:50:55.173396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "object_types = []\n",
    "if case_study == \"bpic17\":\n",
    "    object_types = ['Application', 'Offer', 'Workflow', 'CaseAO', 'CaseAW', 'CaseWO']\n",
    "elif case_study == \"bpic14\":\n",
    "    object_types = [\"ConfigurationItem\", \"Incident\", \"Interaction\", \"Change\"]\n",
    "\n",
    "batch_sizes = {key: 5000 for key in object_types}"
   ],
   "id": "b331d3f8d620514b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:51:12.759216Z",
     "start_time": "2025-05-31T15:50:55.923303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to ensure we don't overload the database, we will run the query for each object_type separately\n",
    "# furthermore, we have a small number of resources (149) with many events, and many cases (31509) with a low number of events\n",
    "# to accommodate for this, we will adapt the batch size accordingly\n",
    "\n",
    "\n",
    "# Discover DF for the objects in our data model\n",
    "df_instance_query_str = '''\n",
    "    CALL apoc.periodic.iterate('\n",
    "        MATCH (o) - [:INSTANCE_OF] -> (ot:ObjectType {objectType: $object_type})\n",
    "        RETURN o, ot\n",
    "    ','\n",
    "        MATCH (e:Event) - [e2o_instance] -> (o) // we remove the check on the event type level for performance increase, and we have no roles so check is redundant\n",
    "        WITH o.id as oId, ot.objectType as oType, e order by e.timestamp, id(e)\n",
    "        WITH oId, oType, collect(e) as events\n",
    "        WITH oId, oType, events\n",
    "        UNWIND range(0, size(events)-2) AS i\n",
    "            WITH oId, oType, events[i] AS e1, events[i+1] AS e2\n",
    "            MERGE (e1) - [df:DF {objectType:oType, id:oId}] -> (e2)',\n",
    "        {batchSize:$batch_size, params:{object_type:$object_type}})\n",
    "'''\n",
    "\n",
    "for object_type in object_types:\n",
    "    print(f\"Discovering df for object_type: {object_type}\")\n",
    "\n",
    "    df_instance_query = Query(\n",
    "        query_str=df_instance_query_str,\n",
    "        parameters={\n",
    "            \"object_type\": object_type,\n",
    "            \"batch_size\": batch_sizes[object_type]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    results = db_connection.exec_query(df_instance_query) \n",
    "    print(results) # ensure failed batches equals 0, if not, consider increasing the memory of the db"
   ],
   "id": "3ba46e75fe853869",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering df for object_type: ConfigurationItem\n",
      "[{'batches': 3, 'total': 14143, 'timeTaken': 0, 'committedOperations': 14143, 'failedOperations': 0, 'failedBatches': 0, 'retries': 0, 'errorMessages': {}, 'batch': {'total': 3, 'errors': {}, 'committed': 3, 'failed': 0}, 'operations': {'total': 14143, 'errors': {}, 'committed': 14143, 'failed': 0}, 'wasTerminated': False, 'failedParams': {}, 'updateStatistics': {'relationshipsDeleted': 0, 'relationshipsCreated': 0, 'nodesDeleted': 0, 'nodesCreated': 0, 'labelsRemoved': 0, 'labelsAdded': 0, 'propertiesSet': 0}}]\n",
      "Discovering df for object_type: Incident\n",
      "[{'batches': 10, 'total': 46616, 'timeTaken': 1, 'committedOperations': 46616, 'failedOperations': 0, 'failedBatches': 0, 'retries': 0, 'errorMessages': {}, 'batch': {'total': 10, 'errors': {}, 'committed': 10, 'failed': 0}, 'operations': {'total': 46616, 'errors': {}, 'committed': 46616, 'failed': 0}, 'wasTerminated': False, 'failedParams': {}, 'updateStatistics': {'relationshipsDeleted': 0, 'relationshipsCreated': 48890, 'nodesDeleted': 0, 'nodesCreated': 0, 'labelsRemoved': 0, 'labelsAdded': 0, 'propertiesSet': 97780}}]\n",
      "Discovering df for object_type: Interaction\n",
      "[{'batches': 30, 'total': 147173, 'timeTaken': 10, 'committedOperations': 147173, 'failedOperations': 0, 'failedBatches': 0, 'retries': 0, 'errorMessages': {}, 'batch': {'total': 30, 'errors': {}, 'committed': 30, 'failed': 0}, 'operations': {'total': 147173, 'errors': {}, 'committed': 147173, 'failed': 0}, 'wasTerminated': False, 'failedParams': {}, 'updateStatistics': {'relationshipsDeleted': 0, 'relationshipsCreated': 760576, 'nodesDeleted': 0, 'nodesCreated': 0, 'labelsRemoved': 0, 'labelsAdded': 0, 'propertiesSet': 1521152}}]\n",
      "Discovering df for object_type: Change\n",
      "[{'batches': 4, 'total': 18026, 'timeTaken': 2, 'committedOperations': 18026, 'failedOperations': 0, 'failedBatches': 0, 'retries': 0, 'errorMessages': {}, 'batch': {'total': 4, 'errors': {}, 'committed': 4, 'failed': 0}, 'operations': {'total': 18026, 'errors': {}, 'committed': 18026, 'failed': 0}, 'wasTerminated': False, 'failedParams': {}, 'updateStatistics': {'relationshipsDeleted': 0, 'relationshipsCreated': 187363, 'nodesDeleted': 0, 'nodesCreated': 0, 'labelsRemoved': 0, 'labelsAdded': 0, 'propertiesSet': 374726}}]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For bpic 17, we have reified entities, so we will find duplicate events\n",
    "\n",
    "As CaseAO, CaseAW and CaseWO are reified entities, we will find duplicate DFs.\n",
    "\n",
    "That is, if we have an `(e1:Event) - [:DF {objectType: 'Offer'}] -> (e2:Event)` and there is no Application events `e3` in between, \n",
    "\n",
    "then we will `(e1:Event) - [:DF {objectType: 'CaseAO'}] -> (e2:Event)`\n",
    "\n",
    "Let's get rid of these duplicate DFs for the reified entities.\n",
    "\n",
    "For BPIC14, running the query does not change anything.\n"
   ],
   "id": "e5ac9ff747842593"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:52:30.555345Z",
     "start_time": "2025-05-31T15:52:30.426750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "delete_duplicate_df = '''\n",
    "    MATCH (from_ot:ObjectType) - [:FROM] -> (reified_ot:ObjectType) - [:TO] -> (to_ot:ObjectType)\n",
    "    WITH from_ot.objectType as from_oType, reified_ot.objectType as reified_oType, to_ot.objectType as to_oType\n",
    "    WITH [from_oType, to_oType] as original_oTypes, reified_oType\n",
    "    UNWIND original_oTypes AS original_oType\n",
    "    MATCH (e1:Event) - [:DF {objectType:original_oType}] -> (e2:Event)\n",
    "    MATCH (e1) - [duplicate_df:DF {objectType:reified_oType}] -> (e2) // check whether there is a duplicate df between e1 and e2\n",
    "    DELETE duplicate_df\n",
    "    RETURN original_oType, reified_oType, count(duplicate_df) as deleted_df\n",
    "'''\n",
    "\n",
    "result = db_connection.exec_query(delete_duplicate_df)\n",
    "print(tabulate(result))"
   ],
   "id": "75ebd0f7766cc44e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we can lift the DF relationships to the event type level to discover a process model.\n",
    "For this, we use a filtered DF-Graph discovery that ensures we only lift the DF relationship if we have seen it often enough"
   ],
   "id": "7faa6721e97fd037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:52:35.301805Z",
     "start_time": "2025-05-31T15:52:32.506332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Query to derive a Multi-Entity DF-Graph by aggregating instance-level DF relationships at the event type level.\n",
    "df_aggregation_query_str = '''\\\n",
    "    CALL apoc.periodic.iterate('\n",
    "        // find all consecutive event types for specific object types\n",
    "        MATCH (e1:Event) - [e2e:DF] -> (e2:Event)\n",
    "        MATCH (e1) - [:INSTANCE_OF] -> (et1:EventType  WHERE et1.agg is null) //ensure not to take the task instances (if they were created with the task analysis)\n",
    "        MATCH (e2) - [:INSTANCE_OF] -> (et2:EventType  WHERE et2.agg is null) //ensure not to take the task instances (if they were created with the task analysis)\n",
    "        WITH e2e.objectType as oType, et1, et2, count(e2e) as df_freq // count for each oType, how often we have observed DF between events that are an instance of et1 and et2\n",
    "        WHERE df_freq > $df_threshold AND oType in $object_types\n",
    "        RETURN oType, et1, et2\n",
    "    ','\n",
    "        WITH oType, et1, et2\n",
    "        MERGE (et1) - [:DF {objectType:oType}] -> (et2)\n",
    "    ', \n",
    "    {batchSize:$batch_size, params:{df_threshold:$df_threshold, object_types:$object_types}})\n",
    "'''\n",
    "\n",
    "df_aggregation_query = Query(\n",
    "    query_str=df_aggregation_query_str,\n",
    "    parameters={\n",
    "        \"df_threshold\": 10 if use_sample else 5_000,\n",
    "        \"object_types\": object_types\n",
    "    }\n",
    ")\n",
    "\n",
    "results = db_connection.exec_query(df_aggregation_query)\n",
    "print(tabulate(results))"
   ],
   "id": "7a0f3f7b0b3521f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  --  -  --  -  -  -  --  -------------------------------------------------------  ---------------------------------------------------------  -----  --  --------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "1  47  2  47  0  0  0  {}  {'total': 1, 'errors': {}, 'committed': 1, 'failed': 0}  {'total': 47, 'errors': {}, 'committed': 47, 'failed': 0}  False  {}  {'relationshipsDeleted': 0, 'relationshipsCreated': 47, 'nodesDeleted': 0, 'nodesCreated': 0, 'labelsRemoved': 0, 'labelsAdded': 0, 'propertiesSet': 47}\n",
      "-  --  -  --  -  -  -  --  -------------------------------------------------------  ---------------------------------------------------------  -----  --  --------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:52:43.651469Z",
     "start_time": "2025-05-31T15:52:35.305055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We should also account for start and end places in our Multi-Entity DFG\n",
    "df_add_start_and_end_places = '''\n",
    "    WITH $object_types as object_types\n",
    "    CALL (object_types) {\n",
    "    \n",
    "    // find all starting events for a specific object type\n",
    "    MATCH (start_e) - [e2o_instance] -> (object) - [:INSTANCE_OF] -> (ot:ObjectType)\n",
    "    WHERE ot.objectType IN object_types\n",
    "    AND NOT EXISTS ((:Event) - [:DF {objectType:ot.objectType, id:object.id}] -> (start_e))\n",
    "    MATCH (start_e) - [:INSTANCE_OF] -> (et2:EventType WHERE et2.agg is null) //ensure not to take the task instances (if they were created with the task analysis)\n",
    "    MERGE (et1:EventType {eventType: \"START\"}) \n",
    "    MERGE (et1) - [:OBSERVES] -> (ot)\n",
    "    RETURN distinct et1, et2, ot.objectType as oType\n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    // find all end events for a specific object type\n",
    "    MATCH (end_e) - [e2o_instance] -> (object) - [:INSTANCE_OF] -> (ot:ObjectType)\n",
    "    WHERE ot.objectType IN object_types\n",
    "    AND NOT EXISTS ((end_e) - [:DF {objectType:ot.objectType, id:object.id}] -> (:Event))\n",
    "    MATCH (end_e) - [:INSTANCE_OF] -> (et1:EventType  WHERE et1.agg is null) //ensure not to take the task instances (if they were created with the task analysis)\n",
    "    MERGE (et2:EventType {eventType: \"END\"}) \n",
    "    MERGE (et2) - [:OBSERVES] -> (ot)\n",
    "    RETURN DISTINCT et1, et2, ot.objectType as oType\n",
    "    }\n",
    "    \n",
    "    MERGE (et1) - [:DF {objectType:oType}] -> (et2)\n",
    "    // print results\n",
    "    WITH DISTINCT oType, et1, et2\n",
    "    RETURN oType as objectType, et1.eventType as from_event_type, et2.eventType as to_event_type order by objectType\n",
    "'''\n",
    "\n",
    "\n",
    "df_add_start_and_end_places_query = Query(\n",
    "    query_str=df_add_start_and_end_places,\n",
    "    parameters={\n",
    "        \"object_types\": object_types\n",
    "    }\n",
    ")\n",
    "\n",
    "results = db_connection.exec_query(df_add_start_and_end_places_query)\n",
    "print(tabulate(results))"
   ],
   "id": "9324f0d9d8a9d785",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------  ---------------------------  ---------------------------\n",
      "Change       START                        Open Change Record\n",
      "Change       START                        Planned Start\n",
      "Change       START                        Start Change Implementation\n",
      "Change       Requested End                END\n",
      "Change       Planned End                  END\n",
      "Change       Close Change Record          END\n",
      "Change       End Change Implementation    END\n",
      "Incident     START                        Open Incident\n",
      "Incident     Close Incident               END\n",
      "Interaction  Update from customer         END\n",
      "Interaction  Operator Update              END\n",
      "Interaction  External Vendor Assignment   END\n",
      "Interaction  Assignment                   END\n",
      "Interaction  Quality Indicator            END\n",
      "Interaction  Vendor Reference             END\n",
      "Interaction  Update                       END\n",
      "Interaction  Close Interaction            END\n",
      "Interaction  Closed                       END\n",
      "Interaction  Caused By CI                 END\n",
      "Interaction  Quality Indicator Fixed      END\n",
      "Interaction  Communication with vendor    END\n",
      "Interaction  Communication with customer  END\n",
      "Interaction  START                        Open\n",
      "Interaction  START                        Closed\n",
      "Interaction  Notify By Change             END\n",
      "-----------  ---------------------------  ---------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4dc217aee6d77b00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
